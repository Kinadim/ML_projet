{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/Kinadim/ML_projet/blob/main/ML_PROJET.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.Analyse des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "Fn0UWiCOWl-w"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\matth\\anaconda3\\lib\\site-packages (3.5.5)\n",
      "Requirement already satisfied: py4j==0.10.9.7 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from pyspark) (0.10.9.7)\n"
     ]
    }
   ],
   "source": [
    "# Importation des diff√©rentes librairies utiles pour le notebook\n",
    "\n",
    "#Sickit learn met r√©guli√®rement √† jour des versions et\n",
    "#indique des futurs warnings.\n",
    "#ces deux lignes permettent de ne pas les afficher.\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "!pip install pyspark\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "import pyspark\n",
    "from pyspark.ml.linalg import Matrices\n",
    "from pyspark.mllib.util import MLUtils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 206
    },
    "id": "geqkifBkYGIk",
    "outputId": "aa15bf3f-36d9-4ad1-e638-0b37f8a3c166"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>science_related</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>316669998137483264</td>\n",
       "      <td>Knees are a bit sore. i guess that's a sign th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>319090866545385472</td>\n",
       "      <td>McDonald's breakfast stop then the gym üèÄüí™</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>322030931022065664</td>\n",
       "      <td>Can any Gynecologist with Cancer Experience ex...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>322694830620807168</td>\n",
       "      <td>Couch-lock highs lead to sleeping in the couch...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>328524426658328576</td>\n",
       "      <td>Does daily routine help prevent problems with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0            tweet_id  \\\n",
       "0           0  316669998137483264   \n",
       "1           1  319090866545385472   \n",
       "2           2  322030931022065664   \n",
       "3           3  322694830620807168   \n",
       "4           4  328524426658328576   \n",
       "\n",
       "                                                text  science_related  \\\n",
       "0  Knees are a bit sore. i guess that's a sign th...                0   \n",
       "1          McDonald's breakfast stop then the gym üèÄüí™                0   \n",
       "2  Can any Gynecologist with Cancer Experience ex...                1   \n",
       "3  Couch-lock highs lead to sleeping in the couch...                1   \n",
       "4  Does daily routine help prevent problems with ...                1   \n",
       "\n",
       "   scientific_claim  scientific_reference  scientific_context  \n",
       "0               0.0                   0.0                 0.0  \n",
       "1               0.0                   0.0                 0.0  \n",
       "2               1.0                   0.0                 0.0  \n",
       "3               1.0                   0.0                 0.0  \n",
       "4               1.0                   0.0                 0.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#attention le s√©parateur est une tabulation\n",
    "df=pd.read_csv('scitweets_export.tsv', sep='\\t')\n",
    "display (df.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Utdgt7JdYR6z",
    "outputId": "5834f869-3f67-4ce3-c7cd-4eeb7726b1ec"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1140 entries, 0 to 1139\n",
      "Data columns (total 7 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   Unnamed: 0            1140 non-null   int64  \n",
      " 1   tweet_id              1140 non-null   int64  \n",
      " 2   text                  1140 non-null   object \n",
      " 3   science_related       1140 non-null   int64  \n",
      " 4   scientific_claim      1140 non-null   float64\n",
      " 5   scientific_reference  1140 non-null   float64\n",
      " 6   scientific_context    1140 non-null   float64\n",
      "dtypes: float64(3), int64(3), object(1)\n",
      "memory usage: 62.5+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "l86S7DRAYoOA"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>science_related</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1.140000e+03</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.000000</td>\n",
       "      <td>1140.00000</td>\n",
       "      <td>1140.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>626.040351</td>\n",
       "      <td>8.560901e+17</td>\n",
       "      <td>0.328947</td>\n",
       "      <td>0.230702</td>\n",
       "      <td>0.17807</td>\n",
       "      <td>0.220175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>364.412255</td>\n",
       "      <td>2.865422e+17</td>\n",
       "      <td>0.470037</td>\n",
       "      <td>0.421467</td>\n",
       "      <td>0.38274</td>\n",
       "      <td>0.414547</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.166700e+17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>306.750000</td>\n",
       "      <td>6.176042e+17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>624.500000</td>\n",
       "      <td>8.615253e+17</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>940.250000</td>\n",
       "      <td>1.103864e+18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1260.000000</td>\n",
       "      <td>1.344485e+18</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0      tweet_id  science_related  scientific_claim  \\\n",
       "count  1140.000000  1.140000e+03      1140.000000       1140.000000   \n",
       "mean    626.040351  8.560901e+17         0.328947          0.230702   \n",
       "std     364.412255  2.865422e+17         0.470037          0.421467   \n",
       "min       0.000000  3.166700e+17         0.000000          0.000000   \n",
       "25%     306.750000  6.176042e+17         0.000000          0.000000   \n",
       "50%     624.500000  8.615253e+17         0.000000          0.000000   \n",
       "75%     940.250000  1.103864e+18         1.000000          0.000000   \n",
       "max    1260.000000  1.344485e+18         1.000000          1.000000   \n",
       "\n",
       "       scientific_reference  scientific_context  \n",
       "count            1140.00000         1140.000000  \n",
       "mean                0.17807            0.220175  \n",
       "std                 0.38274            0.414547  \n",
       "min                 0.00000            0.000000  \n",
       "25%                 0.00000            0.000000  \n",
       "50%                 0.00000            0.000000  \n",
       "75%                 0.00000            0.000000  \n",
       "max                 1.00000            1.000000  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On observe la r√©partition des valeurs pour chaque colonne\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x1000 with 4 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# D√©finir la figure avec 2 lignes et 2 colonnes\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Liste des colonnes √† afficher\n",
    "columns = ['science_related', 'scientific_claim', 'scientific_reference', 'scientific_context']\n",
    "titles = [\n",
    "    \"R√©partition des tweets li√©s √† la science\",\n",
    "    \"R√©partition des tweets avec une affirmation scientifique\",\n",
    "    \"R√©partition des tweets avec une r√©f√©rence scientifique\",\n",
    "    \"R√©partition des tweets de contexte scientifique\"\n",
    "]\n",
    "\n",
    "# G√©n√©rer les 4 countplots avec pourcentage\n",
    "for ax, col, title in zip(axes.flat, columns, titles):\n",
    "    total = len(df[col])  # Nombre total d'observations\n",
    "    sns.barplot(\n",
    "        x=df[col].value_counts(normalize=True).index,  # Valeurs uniques\n",
    "        y=df[col].value_counts(normalize=True).values * 100,  # Pourcentage\n",
    "        ax=ax, palette=\"coolwarm\"\n",
    "    )\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"\")  # Supprimer les labels des axes X pour plus de lisibilit√©\n",
    "    ax.set_ylabel(\"Pourcentage (%)\")\n",
    "\n",
    "# Ajuster l'espacement\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On remarque que la majorit√© des tweets de notre dataset d'entrainement ne sont pas scienfifiques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On regarde la densit√© de valeur pr√©sente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.kdeplot(data=df, fill=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Nettoyage des donn√©es"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a remarqu√© que certains tweets utilisant des mots qui n'√©tait pas en anglais. Nous avons donc trouv√© pour qu'un mod√®le fonctionne, il parait appropri√© de traduire les tweets en anglais. Pour cela nous allons utiliser les fonctionnalit√© de traduction propos√© par Google. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonction pour traduire en anglais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: deep-translator in c:\\users\\matth\\anaconda3\\lib\\site-packages (1.11.4)\n",
      "Requirement already satisfied: beautifulsoup4<5.0.0,>=4.9.1 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from deep-translator) (4.12.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.23.0 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from deep-translator) (2.32.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from beautifulsoup4<5.0.0,>=4.9.1->deep-translator) (2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\matth\\anaconda3\\lib\\site-packages (from requests<3.0.0,>=2.23.0->deep-translator) (2025.1.31)\n",
      "Requirement already satisfied: mlutils in c:\\users\\matth\\anaconda3\\lib\\site-packages (0.2.0b0)\n"
     ]
    }
   ],
   "source": [
    "!pip install deep-translator\n",
    "!pip install mlutils\n",
    "from deep_translator import GoogleTranslator\n",
    "\n",
    "def translate_to_english(tweet):\n",
    "    translated = GoogleTranslator(source='auto', target='en').translate(tweet)\n",
    "    return translated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>science_related</th>\n",
       "      <th>scientific_claim</th>\n",
       "      <th>scientific_reference</th>\n",
       "      <th>scientific_context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>316669998137483264</td>\n",
       "      <td>Knees are a bit sore. i guess that's a sign th...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>319090866545385472</td>\n",
       "      <td>McDonald's breakfast stop then the gym üèÄüí™</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>322030931022065664</td>\n",
       "      <td>Can any Gynecologist with Cancer Experience ex...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>322694830620807168</td>\n",
       "      <td>Couch-lock highs lead to sleeping in the couch...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>328524426658328576</td>\n",
       "      <td>Does daily routine help prevent problems with ...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1135</th>\n",
       "      <td>1255</td>\n",
       "      <td>1340455669443350528</td>\n",
       "      <td>@LaylaFanucci @realDonaldTrump I'm sorry but w...</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1136</th>\n",
       "      <td>1256</td>\n",
       "      <td>1340689510569549824</td>\n",
       "      <td>Dear #NIN applicants, you can kindly download ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1137</th>\n",
       "      <td>1258</td>\n",
       "      <td>1341155832793165825</td>\n",
       "      <td>Whats the uber support team email address?</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1138</th>\n",
       "      <td>1259</td>\n",
       "      <td>1344167355648241664</td>\n",
       "      <td>House passes bill to increase stimulus checks ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1139</th>\n",
       "      <td>1260</td>\n",
       "      <td>1344485313222041600</td>\n",
       "      <td>@berriemomoomin</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1140 rows √ó 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0             tweet_id  \\\n",
       "0              0   316669998137483264   \n",
       "1              1   319090866545385472   \n",
       "2              2   322030931022065664   \n",
       "3              3   322694830620807168   \n",
       "4              4   328524426658328576   \n",
       "...          ...                  ...   \n",
       "1135        1255  1340455669443350528   \n",
       "1136        1256  1340689510569549824   \n",
       "1137        1258  1341155832793165825   \n",
       "1138        1259  1344167355648241664   \n",
       "1139        1260  1344485313222041600   \n",
       "\n",
       "                                                   text  science_related  \\\n",
       "0     Knees are a bit sore. i guess that's a sign th...                0   \n",
       "1             McDonald's breakfast stop then the gym üèÄüí™                0   \n",
       "2     Can any Gynecologist with Cancer Experience ex...                1   \n",
       "3     Couch-lock highs lead to sleeping in the couch...                1   \n",
       "4     Does daily routine help prevent problems with ...                1   \n",
       "...                                                 ...              ...   \n",
       "1135  @LaylaFanucci @realDonaldTrump I'm sorry but w...                1   \n",
       "1136  Dear #NIN applicants, you can kindly download ...                0   \n",
       "1137         Whats the uber support team email address?                0   \n",
       "1138  House passes bill to increase stimulus checks ...                0   \n",
       "1139                                    @berriemomoomin                0   \n",
       "\n",
       "      scientific_claim  scientific_reference  scientific_context  \n",
       "0                  0.0                   0.0                 0.0  \n",
       "1                  0.0                   0.0                 0.0  \n",
       "2                  1.0                   0.0                 0.0  \n",
       "3                  1.0                   0.0                 0.0  \n",
       "4                  1.0                   0.0                 0.0  \n",
       "...                ...                   ...                 ...  \n",
       "1135               1.0                   0.0                 0.0  \n",
       "1136               0.0                   0.0                 0.0  \n",
       "1137               0.0                   0.0                 0.0  \n",
       "1138               0.0                   0.0                 0.0  \n",
       "1139               0.0                   0.0                 0.0  \n",
       "\n",
       "[1140 rows x 7 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df['text'] = df['text'].apply(translate_to_english)\n",
    "display(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\matth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\matth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\matth\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\n    # Mod√®le Decision Tree\\n    model = DecisionTreeClassifier()\\n    model.fit(X_train, y_train)\\n    y_pred = model.predict(X_test)\\n\\n    # √âvaluation\\n    accuracy = accuracy_score(y_test, y_pred)\\n    \\n    # Stocker les r√©sultats\\n    results.append({\"combination\": combination, \"accuracy\": accuracy})\\n    print(f\"Test {i+1}/{len(all_combinations)} - {combination} -> Accuracy: {accuracy:.4f}\")\\n\\n# Afficher les meilleurs r√©sultats\\nresults_df = pd.DataFrame(results)\\n'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import itertools\n",
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from nltk.stem import PorterStemmer # Racinisation des termes\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# T√©l√©charger les ressources n√©cessaires\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Charger les donn√©es\n",
    "\n",
    "# D√©finition des fonctions de pr√©traitement\n",
    "def remove_links(tweet):\n",
    "    return re.sub(r'http\\S+|www\\S+|https\\S+|\\@\\w+|\\#', '', tweet, flags=re.MULTILINE)\n",
    "\n",
    "def remove_mentions(tweet):\n",
    "    return re.sub(r'\\@\\w+', '', tweet, flags=re.MULTILINE)\n",
    "\n",
    "def remove_hashtags(tweet):\n",
    "    return re.sub(r'\\#+', '', tweet, flags=re.MULTILINE)\n",
    "\n",
    "def to_lowercase(tweet):\n",
    "    return tweet.lower()\n",
    "\n",
    "def remove_punctuation(tweet):\n",
    "    return re.sub(r'[^\\w\\s]', '', tweet)\n",
    "\n",
    "def remove_stopwords(tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    return ' '.join([word for word in tokens if word not in stopwords.words('english')])\n",
    "\n",
    "def lemmatize_words(tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in tokens])\n",
    "\n",
    "def stem_words(tweet):\n",
    "    tokens = word_tokenize(tweet)\n",
    "    stemmer = PorterStemmer()\n",
    "    return ' '.join([stemmer.stem(word) for word in tokens])\n",
    "\n",
    "\n",
    "# Liste des transformations possibles\n",
    "transformations = {\n",
    "    \"remove_links\": remove_links,\n",
    "    \"to_lowercase\": to_lowercase,\n",
    "    \"remove_punctuation\": remove_punctuation,\n",
    "    \"remove_stopwords\": remove_stopwords,\n",
    "    \"lemmatize_words\": lemmatize_words,\n",
    "    \"remove_mentions\": remove_mentions,\n",
    "    \"remove_hashtags\": remove_hashtags,\n",
    "    \"stem_words\": stem_words\n",
    "}\n",
    "\n",
    "# G√©n√©rer toutes les combinaisons possibles des transformations\n",
    "all_combinations = []\n",
    "for i in range(1, len(transformations) + 1):\n",
    "    all_combinations.extend(itertools.combinations(transformations.keys(), i))\n",
    "\n",
    "# Stocker les r√©sultats\n",
    "results = []\n",
    "\n",
    "\n",
    "\n",
    "train_test_list=list()\n",
    "\n",
    "# Boucle sur chaque combinaison de transformations\n",
    "for i, combination in enumerate(all_combinations):\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Appliquer les transformations s√©lectionn√©es\n",
    "    for transform in combination:\n",
    "        df_copy['text'] = df_copy['text'].apply(transformations[transform])\n",
    "    \n",
    "    # S√©paration des donn√©es\n",
    "    X = df_copy['text']\n",
    "    y = df_copy['science_related']\n",
    "    \n",
    "    # Vectorisation simple avec TF-IDF\n",
    "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    X_vectorized = vectorizer.fit_transform(X)\n",
    "\n",
    "    # S√©paration en train/test\n",
    "    train_test = train_test_split(X_vectorized, y, test_size=0.2, random_state=42)\n",
    "    clf = make_pipeline(StandardScaler(with_mean=False), SVC(gamma='auto'))\n",
    "    clf.fit(X_vectorized, y)\n",
    "    train_test_list.append((i,combination,train_test,X_vectorized,clf))\n",
    "    \n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    # Mod√®le Decision Tree\n",
    "    model = DecisionTreeClassifier()\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    # √âvaluation\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # Stocker les r√©sultats\n",
    "    results.append({\"combination\": combination, \"accuracy\": accuracy})\n",
    "    print(f\"Test {i+1}/{len(all_combinations)} - {combination} -> Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Afficher les meilleurs r√©sultats\n",
    "results_df = pd.DataFrame(results)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1/255 - ('remove_links',) -> Accuracy: 0.7500\n",
      "Test 2/255 - ('to_lowercase',) -> Accuracy: 0.7763\n",
      "Test 3/255 - ('remove_punctuation',) -> Accuracy: 0.7149\n",
      "Test 4/255 - ('remove_stopwords',) -> Accuracy: 0.7719\n",
      "Test 5/255 - ('lemmatize_words',) -> Accuracy: 0.6930\n",
      "Test 6/255 - ('remove_mentions',) -> Accuracy: 0.7895\n",
      "Test 7/255 - ('remove_hashtags',) -> Accuracy: 0.7763\n",
      "Test 8/255 - ('stem_words',) -> Accuracy: 0.7763\n",
      "Test 9/255 - ('remove_links', 'to_lowercase') -> Accuracy: 0.7500\n",
      "Test 10/255 - ('remove_links', 'remove_punctuation') -> Accuracy: 0.7632\n",
      "Test 11/255 - ('remove_links', 'remove_stopwords') -> Accuracy: 0.7895\n",
      "Test 12/255 - ('remove_links', 'lemmatize_words') -> Accuracy: 0.7237\n",
      "Test 13/255 - ('remove_links', 'remove_mentions') -> Accuracy: 0.7500\n",
      "Test 14/255 - ('remove_links', 'remove_hashtags') -> Accuracy: 0.7500\n",
      "Test 15/255 - ('remove_links', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 16/255 - ('to_lowercase', 'remove_punctuation') -> Accuracy: 0.7149\n",
      "Test 17/255 - ('to_lowercase', 'remove_stopwords') -> Accuracy: 0.7763\n",
      "Test 18/255 - ('to_lowercase', 'lemmatize_words') -> Accuracy: 0.7456\n",
      "Test 19/255 - ('to_lowercase', 'remove_mentions') -> Accuracy: 0.7895\n",
      "Test 20/255 - ('to_lowercase', 'remove_hashtags') -> Accuracy: 0.7763\n",
      "Test 21/255 - ('to_lowercase', 'stem_words') -> Accuracy: 0.7675\n",
      "Test 22/255 - ('remove_punctuation', 'remove_stopwords') -> Accuracy: 0.7807\n",
      "Test 23/255 - ('remove_punctuation', 'lemmatize_words') -> Accuracy: 0.7281\n",
      "Test 24/255 - ('remove_punctuation', 'remove_mentions') -> Accuracy: 0.7149\n",
      "Test 25/255 - ('remove_punctuation', 'remove_hashtags') -> Accuracy: 0.7149\n",
      "Test 26/255 - ('remove_punctuation', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 27/255 - ('remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7675\n",
      "Test 28/255 - ('remove_stopwords', 'remove_mentions') -> Accuracy: 0.7719\n",
      "Test 29/255 - ('remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7719\n",
      "Test 30/255 - ('remove_stopwords', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 31/255 - ('lemmatize_words', 'remove_mentions') -> Accuracy: 0.6930\n",
      "Test 32/255 - ('lemmatize_words', 'remove_hashtags') -> Accuracy: 0.6930\n",
      "Test 33/255 - ('lemmatize_words', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 34/255 - ('remove_mentions', 'remove_hashtags') -> Accuracy: 0.7895\n",
      "Test 35/255 - ('remove_mentions', 'stem_words') -> Accuracy: 0.7588\n",
      "Test 36/255 - ('remove_hashtags', 'stem_words') -> Accuracy: 0.7763\n",
      "Test 37/255 - ('remove_links', 'to_lowercase', 'remove_punctuation') -> Accuracy: 0.7632\n",
      "Test 38/255 - ('remove_links', 'to_lowercase', 'remove_stopwords') -> Accuracy: 0.8114\n",
      "Test 39/255 - ('remove_links', 'to_lowercase', 'lemmatize_words') -> Accuracy: 0.7412\n",
      "Test 40/255 - ('remove_links', 'to_lowercase', 'remove_mentions') -> Accuracy: 0.7500\n",
      "Test 41/255 - ('remove_links', 'to_lowercase', 'remove_hashtags') -> Accuracy: 0.7500\n",
      "Test 42/255 - ('remove_links', 'to_lowercase', 'stem_words') -> Accuracy: 0.7588\n",
      "Test 43/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords') -> Accuracy: 0.7719\n",
      "Test 44/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words') -> Accuracy: 0.7325\n",
      "Test 45/255 - ('remove_links', 'remove_punctuation', 'remove_mentions') -> Accuracy: 0.7632\n",
      "Test 46/255 - ('remove_links', 'remove_punctuation', 'remove_hashtags') -> Accuracy: 0.7632\n",
      "Test 47/255 - ('remove_links', 'remove_punctuation', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 48/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7763\n",
      "Test 49/255 - ('remove_links', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7895\n",
      "Test 50/255 - ('remove_links', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7895\n",
      "Test 51/255 - ('remove_links', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 52/255 - ('remove_links', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7237\n",
      "Test 53/255 - ('remove_links', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7237\n",
      "Test 54/255 - ('remove_links', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 55/255 - ('remove_links', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7500\n",
      "Test 56/255 - ('remove_links', 'remove_mentions', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 57/255 - ('remove_links', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 58/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords') -> Accuracy: 0.7588\n",
      "Test 59/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words') -> Accuracy: 0.7193\n",
      "Test 60/255 - ('to_lowercase', 'remove_punctuation', 'remove_mentions') -> Accuracy: 0.7149\n",
      "Test 61/255 - ('to_lowercase', 'remove_punctuation', 'remove_hashtags') -> Accuracy: 0.7149\n",
      "Test 62/255 - ('to_lowercase', 'remove_punctuation', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 63/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7632\n",
      "Test 64/255 - ('to_lowercase', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7763\n",
      "Test 65/255 - ('to_lowercase', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7763\n",
      "Test 66/255 - ('to_lowercase', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 67/255 - ('to_lowercase', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7456\n",
      "Test 68/255 - ('to_lowercase', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7456\n",
      "Test 69/255 - ('to_lowercase', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 70/255 - ('to_lowercase', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7895\n",
      "Test 71/255 - ('to_lowercase', 'remove_mentions', 'stem_words') -> Accuracy: 0.7675\n",
      "Test 72/255 - ('to_lowercase', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7675\n",
      "Test 73/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7500\n",
      "Test 74/255 - ('remove_punctuation', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7807\n",
      "Test 75/255 - ('remove_punctuation', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7807\n",
      "Test 76/255 - ('remove_punctuation', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7675\n",
      "Test 77/255 - ('remove_punctuation', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7281\n",
      "Test 78/255 - ('remove_punctuation', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7281\n",
      "Test 79/255 - ('remove_punctuation', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 80/255 - ('remove_punctuation', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7149\n",
      "Test 81/255 - ('remove_punctuation', 'remove_mentions', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 82/255 - ('remove_punctuation', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 83/255 - ('remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7675\n",
      "Test 84/255 - ('remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7675\n",
      "Test 85/255 - ('remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 86/255 - ('remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7719\n",
      "Test 87/255 - ('remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 88/255 - ('remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 89/255 - ('lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.6930\n",
      "Test 90/255 - ('lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 91/255 - ('lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 92/255 - ('remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7588\n",
      "Test 93/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords') -> Accuracy: 0.7851\n",
      "Test 94/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words') -> Accuracy: 0.7237\n",
      "Test 95/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_mentions') -> Accuracy: 0.7632\n",
      "Test 96/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_hashtags') -> Accuracy: 0.7632\n",
      "Test 97/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 98/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7500\n",
      "Test 99/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.8114\n",
      "Test 100/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.8114\n",
      "Test 101/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'stem_words') -> Accuracy: 0.8026\n",
      "Test 102/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7412\n",
      "Test 103/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7412\n",
      "Test 104/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 105/255 - ('remove_links', 'to_lowercase', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7500\n",
      "Test 106/255 - ('remove_links', 'to_lowercase', 'remove_mentions', 'stem_words') -> Accuracy: 0.7588\n",
      "Test 107/255 - ('remove_links', 'to_lowercase', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7588\n",
      "Test 108/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7807\n",
      "Test 109/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7719\n",
      "Test 110/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7719\n",
      "Test 111/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7807\n",
      "Test 112/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7325\n",
      "Test 113/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7325\n",
      "Test 114/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 115/255 - ('remove_links', 'remove_punctuation', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7632\n",
      "Test 116/255 - ('remove_links', 'remove_punctuation', 'remove_mentions', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 117/255 - ('remove_links', 'remove_punctuation', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 118/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7763\n",
      "Test 119/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7763\n",
      "Test 120/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 121/255 - ('remove_links', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7895\n",
      "Test 122/255 - ('remove_links', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 123/255 - ('remove_links', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 124/255 - ('remove_links', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7237\n",
      "Test 125/255 - ('remove_links', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 126/255 - ('remove_links', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 127/255 - ('remove_links', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 128/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7544\n",
      "Test 129/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7588\n",
      "Test 130/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7588\n",
      "Test 131/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 132/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7193\n",
      "Test 133/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7193\n",
      "Test 134/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'stem_words') -> Accuracy: 0.6974\n",
      "Test 135/255 - ('to_lowercase', 'remove_punctuation', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7149\n",
      "Test 136/255 - ('to_lowercase', 'remove_punctuation', 'remove_mentions', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 137/255 - ('to_lowercase', 'remove_punctuation', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 138/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7632\n",
      "Test 139/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7632\n",
      "Test 140/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 141/255 - ('to_lowercase', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7763\n",
      "Test 142/255 - ('to_lowercase', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 143/255 - ('to_lowercase', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 144/255 - ('to_lowercase', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7456\n",
      "Test 145/255 - ('to_lowercase', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 146/255 - ('to_lowercase', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 147/255 - ('to_lowercase', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7675\n",
      "Test 148/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7500\n",
      "Test 149/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7500\n",
      "Test 150/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7982\n",
      "Test 151/255 - ('remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7807\n",
      "Test 152/255 - ('remove_punctuation', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7675\n",
      "Test 153/255 - ('remove_punctuation', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7675\n",
      "Test 154/255 - ('remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7281\n",
      "Test 155/255 - ('remove_punctuation', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 156/255 - ('remove_punctuation', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 157/255 - ('remove_punctuation', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 158/255 - ('remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7675\n",
      "Test 159/255 - ('remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 160/255 - ('remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 161/255 - ('remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 162/255 - ('lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 163/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7807\n",
      "Test 164/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7851\n",
      "Test 165/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7851\n",
      "Test 166/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 167/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7237\n",
      "Test 168/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7237\n",
      "Test 169/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 170/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7632\n",
      "Test 171/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_mentions', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 172/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 173/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7500\n",
      "Test 174/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7500\n",
      "Test 175/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 176/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.8114\n",
      "Test 177/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.8026\n",
      "Test 178/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.8026\n",
      "Test 179/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7412\n",
      "Test 180/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 181/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 182/255 - ('remove_links', 'to_lowercase', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7588\n",
      "Test 183/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7807\n",
      "Test 184/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7807\n",
      "Test 185/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 186/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7719\n",
      "Test 187/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7807\n",
      "Test 188/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7807\n",
      "Test 189/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7325\n",
      "Test 190/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 191/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 192/255 - ('remove_links', 'remove_punctuation', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 193/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7763\n",
      "Test 194/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 195/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 196/255 - ('remove_links', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 197/255 - ('remove_links', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 198/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7544\n",
      "Test 199/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7544\n",
      "Test 200/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 201/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7588\n",
      "Test 202/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 203/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 204/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7193\n",
      "Test 205/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.6974\n",
      "Test 206/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.6974\n",
      "Test 207/255 - ('to_lowercase', 'remove_punctuation', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 208/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7632\n",
      "Test 209/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 210/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 211/255 - ('to_lowercase', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 212/255 - ('to_lowercase', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 213/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7500\n",
      "Test 214/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7982\n",
      "Test 215/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7982\n",
      "Test 216/255 - ('remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7675\n",
      "Test 217/255 - ('remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 218/255 - ('remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 219/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7807\n",
      "Test 220/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7807\n",
      "Test 221/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7939\n",
      "Test 222/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7851\n",
      "Test 223/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 224/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 225/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7237\n",
      "Test 226/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 227/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 228/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 229/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7500\n",
      "Test 230/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 231/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 232/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.8026\n",
      "Test 233/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 234/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7807\n",
      "Test 235/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 236/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 237/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7807\n",
      "Test 238/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 239/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 240/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7544\n",
      "Test 241/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 242/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 243/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 244/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.6974\n",
      "Test 245/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 246/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7982\n",
      "Test 247/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7807\n",
      "Test 248/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7939\n",
      "Test 249/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7939\n",
      "Test 250/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 251/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 252/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 253/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 254/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 255/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7939\n"
     ]
    }
   ],
   "source": [
    "result_Decision_Tree_Classifier = []\n",
    "for i in range (len(train_test_list)):\n",
    "    ### Utilisation d'une random pr√©d√©finie pour conserver les r√©sultats\n",
    "    model = DecisionTreeClassifier(random_state=5631)#5631\n",
    "    train_test = train_test_list[i]\n",
    "    train_test_value = train_test[2]\n",
    "    model.fit(train_test_value[0],train_test_value[2])\n",
    "    y_pred = model.predict(train_test_value[1])\n",
    "    accuracy = accuracy_score(train_test_value[3], y_pred)\n",
    "    result_Decision_Tree_Classifier.append({\"combination\": train_test[1], \"accuracy\": accuracy})\n",
    "    print(f\"Test {i+1}/{len(all_combinations)} - {train_test[1]} -> Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 des meilleures combinaisons :\n",
      "                                           combination  accuracy\n",
      "37      (remove_links, to_lowercase, remove_stopwords)  0.811404\n",
      "99   (remove_links, to_lowercase, remove_stopwords,...  0.811404\n",
      "98   (remove_links, to_lowercase, remove_stopwords,...  0.811404\n",
      "175  (remove_links, to_lowercase, remove_stopwords,...  0.811404\n",
      "176  (remove_links, to_lowercase, remove_stopwords,...  0.802632\n",
      "\n",
      "Top 5 des PIRES combinaisons :\n",
      "                                           combination  accuracy\n",
      "133  (to_lowercase, remove_punctuation, lemmatize_w...  0.697368\n",
      "30                  (lemmatize_words, remove_mentions)  0.692982\n",
      "31                  (lemmatize_words, remove_hashtags)  0.692982\n",
      "4                                   (lemmatize_words,)  0.692982\n",
      "88   (lemmatize_words, remove_mentions, remove_hash...  0.692982\n"
     ]
    }
   ],
   "source": [
    "result_dataframe = pd.DataFrame(result_Decision_Tree_Classifier)\n",
    "\n",
    "best_results = result_dataframe.sort_values(by=\"accuracy\", ascending=False).head(5)\n",
    "worst_results = result_dataframe.sort_values(by=\"accuracy\", ascending=False).tail(5)\n",
    "\n",
    "print(\"\\nTop 5 des meilleures combinaisons :\")\n",
    "print(best_results)\n",
    "\n",
    "print(\"\\nTop 5 des PIRES combinaisons :\")\n",
    "print(worst_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Transformation  Average Accuracy\n",
      "3    remove_stopwords          0.768846\n",
      "0        remove_links          0.762815\n",
      "1        to_lowercase          0.760519\n",
      "5     remove_mentions          0.760108\n",
      "7          stem_words          0.759423\n",
      "2  remove_punctuation          0.758703\n",
      "6     remove_hashtags          0.757950\n",
      "4     lemmatize_words          0.751199\n"
     ]
    }
   ],
   "source": [
    "# Calculer la moyenne des accuracy pour chaque transformation\n",
    "average_accuracy = {}\n",
    "for transform in transformations.keys():\n",
    "    accuracies = [result['accuracy'] for result in results if transform in result['combination']]\n",
    "    average_accuracy[transform] = sum(accuracies) / len(accuracies)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "average_accuracy_df = pd.DataFrame(list(average_accuracy.items()), columns=['Transformation', 'Average Accuracy'])\n",
    "average_accuracy_df = average_accuracy_df.sort_values(by='Average Accuracy', ascending=False)\n",
    "print(average_accuracy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1/255 - ('remove_links',) -> Accuracy: 0.7193\n",
      "Test 2/255 - ('to_lowercase',) -> Accuracy: 0.7500\n",
      "Test 3/255 - ('remove_punctuation',) -> Accuracy: 0.7061\n",
      "Test 4/255 - ('remove_stopwords',) -> Accuracy: 0.7456\n",
      "Test 5/255 - ('lemmatize_words',) -> Accuracy: 0.7325\n",
      "Test 6/255 - ('remove_mentions',) -> Accuracy: 0.7105\n",
      "Test 7/255 - ('remove_hashtags',) -> Accuracy: 0.7018\n",
      "Test 8/255 - ('stem_words',) -> Accuracy: 0.7544\n",
      "Test 9/255 - ('remove_links', 'to_lowercase') -> Accuracy: 0.7412\n",
      "Test 10/255 - ('remove_links', 'remove_punctuation') -> Accuracy: 0.7281\n",
      "Test 11/255 - ('remove_links', 'remove_stopwords') -> Accuracy: 0.7588\n",
      "Test 12/255 - ('remove_links', 'lemmatize_words') -> Accuracy: 0.7325\n",
      "Test 13/255 - ('remove_links', 'remove_mentions') -> Accuracy: 0.7325\n",
      "Test 14/255 - ('remove_links', 'remove_hashtags') -> Accuracy: 0.7325\n",
      "Test 15/255 - ('remove_links', 'stem_words') -> Accuracy: 0.7149\n",
      "Test 16/255 - ('to_lowercase', 'remove_punctuation') -> Accuracy: 0.6974\n",
      "Test 17/255 - ('to_lowercase', 'remove_stopwords') -> Accuracy: 0.7105\n",
      "Test 18/255 - ('to_lowercase', 'lemmatize_words') -> Accuracy: 0.7544\n",
      "Test 19/255 - ('to_lowercase', 'remove_mentions') -> Accuracy: 0.7325\n",
      "Test 20/255 - ('to_lowercase', 'remove_hashtags') -> Accuracy: 0.7281\n",
      "Test 21/255 - ('to_lowercase', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 22/255 - ('remove_punctuation', 'remove_stopwords') -> Accuracy: 0.7456\n",
      "Test 23/255 - ('remove_punctuation', 'lemmatize_words') -> Accuracy: 0.7105\n",
      "Test 24/255 - ('remove_punctuation', 'remove_mentions') -> Accuracy: 0.7149\n",
      "Test 25/255 - ('remove_punctuation', 'remove_hashtags') -> Accuracy: 0.7281\n",
      "Test 26/255 - ('remove_punctuation', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 27/255 - ('remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7500\n",
      "Test 28/255 - ('remove_stopwords', 'remove_mentions') -> Accuracy: 0.7193\n",
      "Test 29/255 - ('remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7105\n",
      "Test 30/255 - ('remove_stopwords', 'stem_words') -> Accuracy: 0.7105\n",
      "Test 31/255 - ('lemmatize_words', 'remove_mentions') -> Accuracy: 0.7588\n",
      "Test 32/255 - ('lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7456\n",
      "Test 33/255 - ('lemmatize_words', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 34/255 - ('remove_mentions', 'remove_hashtags') -> Accuracy: 0.7193\n",
      "Test 35/255 - ('remove_mentions', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 36/255 - ('remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 37/255 - ('remove_links', 'to_lowercase', 'remove_punctuation') -> Accuracy: 0.7018\n",
      "Test 38/255 - ('remove_links', 'to_lowercase', 'remove_stopwords') -> Accuracy: 0.7982\n",
      "Test 39/255 - ('remove_links', 'to_lowercase', 'lemmatize_words') -> Accuracy: 0.7018\n",
      "Test 40/255 - ('remove_links', 'to_lowercase', 'remove_mentions') -> Accuracy: 0.7281\n",
      "Test 41/255 - ('remove_links', 'to_lowercase', 'remove_hashtags') -> Accuracy: 0.7193\n",
      "Test 42/255 - ('remove_links', 'to_lowercase', 'stem_words') -> Accuracy: 0.7105\n",
      "Test 43/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords') -> Accuracy: 0.7368\n",
      "Test 44/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words') -> Accuracy: 0.7193\n",
      "Test 45/255 - ('remove_links', 'remove_punctuation', 'remove_mentions') -> Accuracy: 0.7325\n",
      "Test 46/255 - ('remove_links', 'remove_punctuation', 'remove_hashtags') -> Accuracy: 0.7061\n",
      "Test 47/255 - ('remove_links', 'remove_punctuation', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 48/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7368\n",
      "Test 49/255 - ('remove_links', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7281\n",
      "Test 50/255 - ('remove_links', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7500\n",
      "Test 51/255 - ('remove_links', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 52/255 - ('remove_links', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7325\n",
      "Test 53/255 - ('remove_links', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7456\n",
      "Test 54/255 - ('remove_links', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 55/255 - ('remove_links', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7325\n",
      "Test 56/255 - ('remove_links', 'remove_mentions', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 57/255 - ('remove_links', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 58/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords') -> Accuracy: 0.7412\n",
      "Test 59/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words') -> Accuracy: 0.6930\n",
      "Test 60/255 - ('to_lowercase', 'remove_punctuation', 'remove_mentions') -> Accuracy: 0.7105\n",
      "Test 61/255 - ('to_lowercase', 'remove_punctuation', 'remove_hashtags') -> Accuracy: 0.7105\n",
      "Test 62/255 - ('to_lowercase', 'remove_punctuation', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 63/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7456\n",
      "Test 64/255 - ('to_lowercase', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7061\n",
      "Test 65/255 - ('to_lowercase', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7325\n",
      "Test 66/255 - ('to_lowercase', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 67/255 - ('to_lowercase', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7368\n",
      "Test 68/255 - ('to_lowercase', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7412\n",
      "Test 69/255 - ('to_lowercase', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 70/255 - ('to_lowercase', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7281\n",
      "Test 71/255 - ('to_lowercase', 'remove_mentions', 'stem_words') -> Accuracy: 0.7105\n",
      "Test 72/255 - ('to_lowercase', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 73/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7325\n",
      "Test 74/255 - ('remove_punctuation', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7237\n",
      "Test 75/255 - ('remove_punctuation', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7456\n",
      "Test 76/255 - ('remove_punctuation', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 77/255 - ('remove_punctuation', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7193\n",
      "Test 78/255 - ('remove_punctuation', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7325\n",
      "Test 79/255 - ('remove_punctuation', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 80/255 - ('remove_punctuation', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7193\n",
      "Test 81/255 - ('remove_punctuation', 'remove_mentions', 'stem_words') -> Accuracy: 0.7193\n",
      "Test 82/255 - ('remove_punctuation', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 83/255 - ('remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7368\n",
      "Test 84/255 - ('remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7237\n",
      "Test 85/255 - ('remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 86/255 - ('remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7149\n",
      "Test 87/255 - ('remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 88/255 - ('remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 89/255 - ('lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7544\n",
      "Test 90/255 - ('lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 91/255 - ('lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 92/255 - ('remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 93/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords') -> Accuracy: 0.7675\n",
      "Test 94/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words') -> Accuracy: 0.7412\n",
      "Test 95/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_mentions') -> Accuracy: 0.7237\n",
      "Test 96/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_hashtags') -> Accuracy: 0.7412\n",
      "Test 97/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 98/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7763\n",
      "Test 99/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7763\n",
      "Test 100/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7675\n",
      "Test 101/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7719\n",
      "Test 102/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7500\n",
      "Test 103/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7281\n",
      "Test 104/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7325\n",
      "Test 105/255 - ('remove_links', 'to_lowercase', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7105\n",
      "Test 106/255 - ('remove_links', 'to_lowercase', 'remove_mentions', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 107/255 - ('remove_links', 'to_lowercase', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 108/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7456\n",
      "Test 109/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7500\n",
      "Test 110/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7325\n",
      "Test 111/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7149\n",
      "Test 112/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7368\n",
      "Test 113/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7368\n",
      "Test 114/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 115/255 - ('remove_links', 'remove_punctuation', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7412\n",
      "Test 116/255 - ('remove_links', 'remove_punctuation', 'remove_mentions', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 117/255 - ('remove_links', 'remove_punctuation', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7325\n",
      "Test 118/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7632\n",
      "Test 119/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7544\n",
      "Test 120/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 121/255 - ('remove_links', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7368\n",
      "Test 122/255 - ('remove_links', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 123/255 - ('remove_links', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 124/255 - ('remove_links', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7719\n",
      "Test 125/255 - ('remove_links', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 126/255 - ('remove_links', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7193\n",
      "Test 127/255 - ('remove_links', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 128/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7237\n",
      "Test 129/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7281\n",
      "Test 130/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7368\n",
      "Test 131/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7325\n",
      "Test 132/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7544\n",
      "Test 133/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7149\n",
      "Test 134/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 135/255 - ('to_lowercase', 'remove_punctuation', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7018\n",
      "Test 136/255 - ('to_lowercase', 'remove_punctuation', 'remove_mentions', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 137/255 - ('to_lowercase', 'remove_punctuation', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7149\n",
      "Test 138/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7500\n",
      "Test 139/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7412\n",
      "Test 140/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 141/255 - ('to_lowercase', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7061\n",
      "Test 142/255 - ('to_lowercase', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 143/255 - ('to_lowercase', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 144/255 - ('to_lowercase', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7412\n",
      "Test 145/255 - ('to_lowercase', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7325\n",
      "Test 146/255 - ('to_lowercase', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 147/255 - ('to_lowercase', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7325\n",
      "Test 148/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7368\n",
      "Test 149/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7456\n",
      "Test 150/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 151/255 - ('remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7368\n",
      "Test 152/255 - ('remove_punctuation', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 153/255 - ('remove_punctuation', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7325\n",
      "Test 154/255 - ('remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7193\n",
      "Test 155/255 - ('remove_punctuation', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7149\n",
      "Test 156/255 - ('remove_punctuation', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7325\n",
      "Test 157/255 - ('remove_punctuation', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7149\n",
      "Test 158/255 - ('remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7412\n",
      "Test 159/255 - ('remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 160/255 - ('remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 161/255 - ('remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 162/255 - ('lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 163/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words') -> Accuracy: 0.7719\n",
      "Test 164/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions') -> Accuracy: 0.7807\n",
      "Test 165/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_hashtags') -> Accuracy: 0.7719\n",
      "Test 166/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'stem_words') -> Accuracy: 0.7719\n",
      "Test 167/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7237\n",
      "Test 168/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7193\n",
      "Test 169/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 170/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7281\n",
      "Test 171/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_mentions', 'stem_words') -> Accuracy: 0.7325\n",
      "Test 172/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 173/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7807\n",
      "Test 174/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7895\n",
      "Test 175/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7807\n",
      "Test 176/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7807\n",
      "Test 177/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 178/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 179/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7281\n",
      "Test 180/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 181/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 182/255 - ('remove_links', 'to_lowercase', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 183/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7281\n",
      "Test 184/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7368\n",
      "Test 185/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 186/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7632\n",
      "Test 187/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 188/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 189/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7105\n",
      "Test 190/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 191/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 192/255 - ('remove_links', 'remove_punctuation', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 193/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7588\n",
      "Test 194/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 195/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 196/255 - ('remove_links', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 197/255 - ('remove_links', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7588\n",
      "Test 198/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7456\n",
      "Test 199/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7368\n",
      "Test 200/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 201/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7588\n",
      "Test 202/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 203/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 204/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7149\n",
      "Test 205/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 206/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7193\n",
      "Test 207/255 - ('to_lowercase', 'remove_punctuation', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 208/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7544\n",
      "Test 209/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7325\n",
      "Test 210/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7149\n",
      "Test 211/255 - ('to_lowercase', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 212/255 - ('to_lowercase', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 213/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7237\n",
      "Test 214/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 215/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7325\n",
      "Test 216/255 - ('remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 217/255 - ('remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7237\n",
      "Test 218/255 - ('remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 219/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions') -> Accuracy: 0.7719\n",
      "Test 220/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags') -> Accuracy: 0.7807\n",
      "Test 221/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'stem_words') -> Accuracy: 0.7807\n",
      "Test 222/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.8026\n",
      "Test 223/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'stem_words') -> Accuracy: 0.7719\n",
      "Test 224/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 225/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7281\n",
      "Test 226/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 227/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 228/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7719\n",
      "Test 229/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7763\n",
      "Test 230/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7939\n",
      "Test 231/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7632\n",
      "Test 232/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 233/255 - ('remove_links', 'to_lowercase', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7588\n",
      "Test 234/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7456\n",
      "Test 235/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7500\n",
      "Test 236/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 237/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 238/255 - ('remove_links', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7544\n",
      "Test 239/255 - ('remove_links', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 240/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7237\n",
      "Test 241/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7588\n",
      "Test 242/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 243/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7368\n",
      "Test 244/255 - ('to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7588\n",
      "Test 245/255 - ('to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7412\n",
      "Test 246/255 - ('remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 247/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags') -> Accuracy: 0.7807\n",
      "Test 248/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'stem_words') -> Accuracy: 0.7895\n",
      "Test 249/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_hashtags', 'stem_words') -> Accuracy: 0.8026\n",
      "Test 250/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7675\n",
      "Test 251/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7456\n",
      "Test 252/255 - ('remove_links', 'to_lowercase', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7807\n",
      "Test 253/255 - ('remove_links', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7281\n",
      "Test 254/255 - ('to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.7325\n",
      "Test 255/255 - ('remove_links', 'to_lowercase', 'remove_punctuation', 'remove_stopwords', 'lemmatize_words', 'remove_mentions', 'remove_hashtags', 'stem_words') -> Accuracy: 0.8026\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "result_RandomForestClassifier_Classifier = []\n",
    "for i in range (len(train_test_list)):\n",
    "    ### Utilisation d'une random pr√©d√©finie pour conserver les r√©sultats\n",
    "    model = RandomForestClassifier(45)\n",
    "    train_test = train_test_list[i]\n",
    "    train_test_value = train_test[2]\n",
    "    model.fit(train_test_value[0],train_test_value[2])\n",
    "    y_pred = model.predict(train_test_value[1])\n",
    "    accuracy = accuracy_score(train_test_value[3], y_pred)\n",
    "    result_RandomForestClassifier_Classifier.append({\"combination\": train_test[1], \"accuracy\": accuracy})\n",
    "    print(f\"Test {i+1}/{len(all_combinations)} - {train_test[1]} -> Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 des meilleures combinaisons :\n",
      "                                           combination  accuracy\n",
      "254  (remove_links, to_lowercase, remove_punctuatio...  0.802632\n",
      "221  (remove_links, to_lowercase, remove_punctuatio...  0.802632\n",
      "248  (remove_links, to_lowercase, remove_punctuatio...  0.802632\n",
      "37      (remove_links, to_lowercase, remove_stopwords)  0.798246\n",
      "229  (remove_links, to_lowercase, remove_stopwords,...  0.793860\n",
      "\n",
      "Top 5 des PIRES combinaisons :\n",
      "                                           combination  accuracy\n",
      "36    (remove_links, to_lowercase, remove_punctuation)  0.701754\n",
      "6                                   (remove_hashtags,)  0.701754\n",
      "134  (to_lowercase, remove_punctuation, remove_ment...  0.701754\n",
      "15                  (to_lowercase, remove_punctuation)  0.697368\n",
      "58   (to_lowercase, remove_punctuation, lemmatize_w...  0.692982\n"
     ]
    }
   ],
   "source": [
    "result_dataframe = pd.DataFrame(result_RandomForestClassifier_Classifier)\n",
    "\n",
    "best_results = result_dataframe.sort_values(by=\"accuracy\", ascending=False).head(5)\n",
    "worst_results = result_dataframe.sort_values(by=\"accuracy\", ascending=False).tail(5)\n",
    "\n",
    "print(\"\\nTop 5 des meilleures combinaisons :\")\n",
    "print(best_results)\n",
    "\n",
    "print(\"\\nTop 5 des PIRES combinaisons :\")\n",
    "print(worst_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Transformation  Average Accuracy\n",
      "3    remove_stopwords          0.768846\n",
      "0        remove_links          0.762815\n",
      "1        to_lowercase          0.760519\n",
      "5     remove_mentions          0.760108\n",
      "7          stem_words          0.759423\n",
      "2  remove_punctuation          0.758703\n",
      "6     remove_hashtags          0.757950\n",
      "4     lemmatize_words          0.751199\n"
     ]
    }
   ],
   "source": [
    "# Calculer la moyenne des accuracy pour chaque transformation\n",
    "average_accuracy = {}\n",
    "for transform in transformations.keys():\n",
    "    accuracies = [result['accuracy'] for result in results if transform in result['combination']]\n",
    "    average_accuracy[transform] = sum(accuracies) / len(accuracies)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "average_accuracy_df = pd.DataFrame(list(average_accuracy.items()), columns=['Transformation', 'Average Accuracy'])\n",
    "average_accuracy_df = average_accuracy_df.sort_values(by='Average Accuracy', ascending=False)\n",
    "print(average_accuracy_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Support Vector Machine (SVC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test 1/255 - ('remove_links',) -> Accuracy: 0.6787\n",
      "Test 2/255 - ('to_lowercase',) -> Accuracy: 0.6787\n",
      "Test 3/255 - ('remove_punctuation',) -> Accuracy: 0.6787\n",
      "Test 4/255 - ('remove_stopwords',) -> Accuracy: 0.6787\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[93], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m train_test_value \u001b[38;5;241m=\u001b[39m train_test[\u001b[38;5;241m2\u001b[39m]\n\u001b[0;32m      9\u001b[0m model \u001b[38;5;241m=\u001b[39m train_test[\u001b[38;5;241m4\u001b[39m]\n\u001b[1;32m---> 10\u001b[0m scores \u001b[38;5;241m=\u001b[39m \u001b[43mcross_val_score\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_test_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_test_value\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m accuracy \u001b[38;5;241m=\u001b[39m scores\u001b[38;5;241m.\u001b[39mmean()\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m#y_pred = model.predict(train_test_value[1])\u001b[39;00m\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m#accuracy = accuracy_score(train_test_value[3], y_pred)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:712\u001b[0m, in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;66;03m# To ensure multimetric format is not supported\u001b[39;00m\n\u001b[0;32m    710\u001b[0m scorer \u001b[38;5;241m=\u001b[39m check_scoring(estimator, scoring\u001b[38;5;241m=\u001b[39mscoring)\n\u001b[1;32m--> 712\u001b[0m cv_results \u001b[38;5;241m=\u001b[39m \u001b[43mcross_validate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    713\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    714\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    715\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    716\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgroups\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mscoring\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mscore\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mscorer\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m    \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    723\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpre_dispatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpre_dispatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    724\u001b[0m \u001b[43m    \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    725\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    726\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m cv_results[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_score\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:213\u001b[0m, in \u001b[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    207\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    208\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m    209\u001b[0m         skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m    210\u001b[0m             prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m    211\u001b[0m         )\n\u001b[0;32m    212\u001b[0m     ):\n\u001b[1;32m--> 213\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m InvalidParameterError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    215\u001b[0m     \u001b[38;5;66;03m# When the function is just a wrapper around an estimator, we allow\u001b[39;00m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;66;03m# the function to delegate validation to the estimator, but we replace\u001b[39;00m\n\u001b[0;32m    217\u001b[0m     \u001b[38;5;66;03m# the name of the estimator by the name of the function in the error\u001b[39;00m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;66;03m# message to avoid confusion.\u001b[39;00m\n\u001b[0;32m    219\u001b[0m     msg \u001b[38;5;241m=\u001b[39m re\u001b[38;5;241m.\u001b[39msub(\n\u001b[0;32m    220\u001b[0m         \u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;124m\\\u001b[39m\u001b[38;5;124mw+ must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    221\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mparameter of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must be\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    222\u001b[0m         \u001b[38;5;28mstr\u001b[39m(e),\n\u001b[0;32m    223\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:423\u001b[0m, in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, params, pre_dispatch, return_train_score, return_estimator, return_indices, error_score)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[38;5;66;03m# We clone the estimator to make sure that all the folds are\u001b[39;00m\n\u001b[0;32m    421\u001b[0m \u001b[38;5;66;03m# independent, and that it is pickle-able.\u001b[39;00m\n\u001b[0;32m    422\u001b[0m parallel \u001b[38;5;241m=\u001b[39m Parallel(n_jobs\u001b[38;5;241m=\u001b[39mn_jobs, verbose\u001b[38;5;241m=\u001b[39mverbose, pre_dispatch\u001b[38;5;241m=\u001b[39mpre_dispatch)\n\u001b[1;32m--> 423\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    425\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    426\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    427\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    428\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscorers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    429\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    430\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    431\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    432\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    433\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfit_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    434\u001b[0m \u001b[43m        \u001b[49m\u001b[43mscore_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrouted_params\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscorer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscore\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    435\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_train_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_train_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    436\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_times\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    437\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreturn_estimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_estimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43merror_score\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merror_score\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    440\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\n\u001b[0;32m    441\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    443\u001b[0m _warn_or_raise_about_fit_failures(results, error_score)\n\u001b[0;32m    445\u001b[0m \u001b[38;5;66;03m# For callable scoring, the return type is only know after calling. If the\u001b[39;00m\n\u001b[0;32m    446\u001b[0m \u001b[38;5;66;03m# return type is a dictionary, the error scores can now be inserted with\u001b[39;00m\n\u001b[0;32m    447\u001b[0m \u001b[38;5;66;03m# the correct key.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:74\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     69\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     70\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     71\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     73\u001b[0m )\n\u001b[1;32m---> 74\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1918\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1916\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[0;32m   1917\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[1;32m-> 1918\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1920\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[0;32m   1921\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[0;32m   1922\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[0;32m   1923\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[0;32m   1924\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[0;32m   1925\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\joblib\\parallel.py:1847\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1845\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1846\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m-> 1847\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\utils\\parallel.py:136\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    134\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[1;32m--> 136\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\model_selection\\_validation.py:888\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, score_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    886\u001b[0m         estimator\u001b[38;5;241m.\u001b[39mfit(X_train, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mfit_params)\n\u001b[0;32m    887\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 888\u001b[0m         \u001b[43mestimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m    891\u001b[0m     \u001b[38;5;66;03m# Note fit time as time until error\u001b[39;00m\n\u001b[0;32m    892\u001b[0m     fit_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\pipeline.py:473\u001b[0m, in \u001b[0;36mPipeline.fit\u001b[1;34m(self, X, y, **params)\u001b[0m\n\u001b[0;32m    471\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_final_estimator \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpassthrough\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    472\u001b[0m         last_step_params \u001b[38;5;241m=\u001b[39m routed_params[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m]]\n\u001b[1;32m--> 473\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_final_estimator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mlast_step_params\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfit\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1471\u001b[0m     )\n\u001b[0;32m   1472\u001b[0m ):\n\u001b[1;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:250\u001b[0m, in \u001b[0;36mBaseLibSVM.fit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    247\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LibSVM]\u001b[39m\u001b[38;5;124m\"\u001b[39m, end\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    249\u001b[0m seed \u001b[38;5;241m=\u001b[39m rnd\u001b[38;5;241m.\u001b[39mrandint(np\u001b[38;5;241m.\u001b[39miinfo(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mi\u001b[39m\u001b[38;5;124m\"\u001b[39m)\u001b[38;5;241m.\u001b[39mmax)\n\u001b[1;32m--> 250\u001b[0m \u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    251\u001b[0m \u001b[38;5;66;03m# see comment on the other call to np.iinfo in this file\u001b[39;00m\n\u001b[0;32m    253\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape_fit_ \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mshape \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshape\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m (n_samples,)\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\sklearn\\svm\\_base.py:369\u001b[0m, in \u001b[0;36mBaseLibSVM._sparse_fit\u001b[1;34m(self, X, y, sample_weight, solver_type, kernel, random_seed)\u001b[0m\n\u001b[0;32m    355\u001b[0m kernel_type \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sparse_kernels\u001b[38;5;241m.\u001b[39mindex(kernel)\n\u001b[0;32m    357\u001b[0m libsvm_sparse\u001b[38;5;241m.\u001b[39mset_verbosity_wrap(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose)\n\u001b[0;32m    359\u001b[0m (\n\u001b[0;32m    360\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_,\n\u001b[0;32m    361\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msupport_vectors_,\n\u001b[0;32m    362\u001b[0m     dual_coef_data,\n\u001b[0;32m    363\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_,\n\u001b[0;32m    364\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_support,\n\u001b[0;32m    365\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probA,\n\u001b[0;32m    366\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_probB,\n\u001b[0;32m    367\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfit_status_,\n\u001b[0;32m    368\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_iter,\n\u001b[1;32m--> 369\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[43mlibsvm_sparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlibsvm_sparse_train\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    370\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    371\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    372\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    373\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindptr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    374\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    375\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    376\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkernel_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    377\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdegree\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    378\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gamma\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    379\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcoef0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    380\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    381\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    382\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mclass_weight_\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    383\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    384\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnu\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    385\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcache_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    386\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    387\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshrinking\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    388\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobability\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    389\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    390\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    391\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    393\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_warn_from_fit_status()\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclasses_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[1;32m_libsvm_sparse.pyx:218\u001b[0m, in \u001b[0;36msklearn.svm._libsvm_sparse.libsvm_sparse_train\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\matth\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:27\u001b[0m, in \u001b[0;36m_cs_matrix.__init__\u001b[1;34m(self, arg1, shape, dtype, copy)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_cs_matrix\u001b[39;00m(_data_matrix, _minmax_mixin, IndexMixin):\n\u001b[0;32m     23\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;124;03m    base array/matrix class for compressed row- and column-oriented arrays/matrices\u001b[39;00m\n\u001b[0;32m     25\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, arg1, shape\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     28\u001b[0m         _data_matrix\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m)\n\u001b[0;32m     30\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m issparse(arg1):\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "result_SVC_Classifier = []\n",
    "for i in range (len(train_test_list)):\n",
    "    ### Utilisation d'une random pr√©d√©finie pour conserver les r√©sultats\n",
    "    #model =  SVC()#5631\n",
    "    train_test = train_test_list[i]\n",
    "    train_test_value = train_test[2]\n",
    "    model = train_test[4]\n",
    "    scores = cross_val_score(model, train_test_value[0], train_test_value[2], cv=5)\n",
    "    accuracy = scores.mean()\n",
    "    #y_pred = model.predict(train_test_value[1])\n",
    "    #accuracy = accuracy_score(train_test_value[3], y_pred)\n",
    "    result_SVC_Classifier.append({\"combination\": train_test[1], \"accuracy\": accuracy})\n",
    "    print(f\"Test {i+1}/{len(all_combinations)} - {train_test[1]} -> Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 des meilleures combinaisons :\n",
      "                                           combination  accuracy\n",
      "105  (remove_links, to_lowercase, remove_mentions, ...  0.679829\n",
      "56         (remove_links, remove_hashtags, stem_words)  0.679829\n",
      "115  (remove_links, remove_punctuation, remove_ment...  0.679829\n",
      "232  (remove_links, to_lowercase, lemmatize_words, ...  0.679829\n",
      "113  (remove_links, remove_punctuation, lemmatize_w...  0.679829\n",
      "\n",
      "Top 5 des PIRES combinaisons :\n",
      "                                           combination  accuracy\n",
      "90      (lemmatize_words, remove_hashtags, stem_words)  0.678731\n",
      "91      (remove_mentions, remove_hashtags, stem_words)  0.678731\n",
      "92   (remove_links, to_lowercase, remove_punctuatio...  0.678731\n",
      "93   (remove_links, to_lowercase, remove_punctuatio...  0.678731\n",
      "254  (remove_links, to_lowercase, remove_punctuatio...  0.678731\n"
     ]
    }
   ],
   "source": [
    "result_dataframe = pd.DataFrame(result_SVC_Classifier)\n",
    "\n",
    "best_results = result_dataframe.sort_values(by=\"accuracy\", ascending=False).head(5)\n",
    "worst_results = result_dataframe.sort_values(by=\"accuracy\", ascending=False).tail(5)\n",
    "\n",
    "print(\"\\nTop 5 des meilleures combinaisons :\")\n",
    "print(best_results)\n",
    "\n",
    "print(\"\\nTop 5 des PIRES combinaisons :\")\n",
    "print(worst_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       Transformation  Average Accuracy\n",
      "3    remove_stopwords          0.768846\n",
      "0        remove_links          0.762815\n",
      "1        to_lowercase          0.760519\n",
      "5     remove_mentions          0.760108\n",
      "7          stem_words          0.759423\n",
      "2  remove_punctuation          0.758703\n",
      "6     remove_hashtags          0.757950\n",
      "4     lemmatize_words          0.751199\n"
     ]
    }
   ],
   "source": [
    "# Calculer la moyenne des accuracy pour chaque transformation\n",
    "average_accuracy = {}\n",
    "for transform in transformations.keys():\n",
    "    accuracies = [result['accuracy'] for result in results if transform in result['combination']]\n",
    "    average_accuracy[transform] = sum(accuracies) / len(accuracies)\n",
    "\n",
    "# Afficher les r√©sultats\n",
    "average_accuracy_df = pd.DataFrame(list(average_accuracy.items()), columns=['Transformation', 'Average Accuracy'])\n",
    "average_accuracy_df = average_accuracy_df.sort_values(by='Average Accuracy', ascending=False)\n",
    "print(average_accuracy_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.77\n",
      "\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.84      0.83       146\n",
      "           1       0.70      0.65      0.67        82\n",
      "\n",
      "    accuracy                           0.77       228\n",
      "   macro avg       0.75      0.74      0.75       228\n",
      "weighted avg       0.77      0.77      0.77       228\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "# Transformation du texte en vecteurs TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)  # Limite √† 5000 mots pour √©viter la surcharge\n",
    "X = vectorizer.fit_transform(df['text'])\n",
    "\n",
    "# D√©finition de la variable cible\n",
    "y = df['science_related']\n",
    "\n",
    "# S√©paration des donn√©es en train et test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Cr√©ation et entra√Ænement du mod√®le\n",
    "model = DecisionTreeClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Pr√©diction sur les donn√©es test\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# √âvaluation du mod√®le\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice de confusion :\n",
      "[[123  23]\n",
      " [ 29  53]]\n",
      "\n",
      "Vrais N√©gatifs  (TN) : 123\n",
      "Faux Positifs  (FP) : 23\n",
      "Faux N√©gatifs  (FN) : 29\n",
      "Vrais Positifs  (TP) : 53\n"
     ]
    }
   ],
   "source": [
    "# Calcul de la matrice de confusion\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Affichage simple\n",
    "print(\"Matrice de confusion :\")\n",
    "print(cm)\n",
    "\n",
    "# Explication des valeurs\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "print(f\"\\nVrais N√©gatifs  (TN) : {tn}\")\n",
    "print(f\"Faux Positifs  (FP) : {fp}\")\n",
    "print(f\"Faux N√©gatifs  (FN) : {fn}\")\n",
    "print(f\"Vrais Positifs  (TP) : {tp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Beaucoup de TP & TN ‚Üí Mod√®le performant-> valeurs r√©elles bien guess\n",
    "Beaucoup de FP ‚Üí Mod√®le fait trop d'erreurs en classant des tweets non scientifiques comme scientifiques\n",
    "Beaucoup de FN ‚Üí Mod√®le rate des tweets scientifiques"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "include_colab_link": true,
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
